# 과적합과 모델 선택



## 기댓값과 분산



기댓값 : 확률변수가 취하는 값을 확률로 가중치를 둔 평균값

분산 : 확률변수가 취하는 값이 어느정도 퍼져 있는지 나타낸 것

정규분포 : 기댓값을 중심으로 좌우 대칭인 종모양을 띄고 있다.



회귀문제를 낳는 실제 함수를 아래와 같다고 할 때,
$$
y=f_t(x)+ε
$$
실제 함수 F는 알 수 없으므로, 추정한 모델 f를 사용해 예측한다.
$$
y_e=f_e(x)
$$


학습데이터를 생성한 실제함수가 공통이라고 해도, 노이즈(ε)에 따라서 매번 같아진다고 할 수 ❌



실제 함수가 공통이라도 각 데이터의 세부 모양은 노이즈의 영향으로 달라진다. 이 노이즈의 영향으로 데이터에 의해 추정되는 f도 흩어진다.

이 흩어진 정도를 평가하기 위해 학습 데이터간의 분산을 계산한다.
$$
Var[f_e(x_0) ]
$$


## 편향-분산 분해

과적합을 이해할 때 편향-분산 분해는 중요한 역할을 한다.

이것을 편향-분산 분해로 부른다.


$$
E[(y_0-f_e(x_0))^2] = (f_t(x_0)-E[f_e(x_0)])^2 + Var[f_e(x_0)] + Var[ε]
$$


기대 MSE(기대 평균 제곱오차)  = 편향의 제곱(근사오차) + 분산 + 노이즈

추정 함수가 실제함수를 표현하는 데 유연성이 결여된 함수라면, 아무리 고생해서 모델을 학습해도 이부분은 양수가 된다.

반대로 추정함수가 비모수적이고 유연한 방법인 경우는 기본적으로 편향이 0에 가까워진다.



2항인 분산은 추정함수가 유연해질수록 높아진다.  다항식모델의 차수가 높아지면, 즉 유연한 함수가 되면 될수록 분산이 높아진다.



3항은 랜덤 노이즈의 분산으로 이부분은 추정함수와는 관계없이 존재하며, 어떤 추정 기법을 써도 줄일 수 없다.





## 편향 - 분산 트레이드오프



![err3.png](https://steemitimages.com/DQmWkRGtwfxHB3chPEx5vxFaU3dX1W31yZxgmPiaWXrioyU/err3.png)

**(사진출처 :** https://steemit.com/kr/@doctorbme/doctorbme-essay-bias-variance)



**편향 - 분산 트레이드오프**는 추정함수의 유연성을 높여 근사오차를 낮추려고 할수록 배리언스가 상승하는 상관관계를 가리킨다.

복잡한 모델일수록 편향은 낮출 수 있지만 분산이 높아지므로, 반드시 간단한 모델보다 예측 정밀도가 뛰어난 것은 아니다.



## 교차검증법

시험 데이터를 구할 수 없는 경우,  의사적으로 학습 데이터와 시험 데이터를 나누는 게 일반적이다. 이 발상을 단순하게 확장해서 우선 N개의 데이터를 K 분할하고 K-1개의 데이터를 학습데이ㅓ로 취급해 모델을 추정한다.

다음으로 여기서 얻어진 모델을 이용해 나머지 하나를 시험데이터로 간주하고 평균오차를 측정한다. 이 과정을 K번 반복해 평균오차의 평균값을 취하면 시험 오류의 근삿값을 계산할 수 있습니다.  이 방법을 **교차검증법**이라고 한다.

학습할 때와 시험할 때 데이터를 생성하는 배후 모델이 같다고 가정할 수 있으면 어떤 상황에서도 사용할 수 있다.

![img](https://blog.kakaocdn.net/dn/bNqj2c/btqZqpGBQcc/QqkKbQkuTfXrctdlUTpxl0/img.png)

(사진출처:https://bbdata.tistory.com/10)





[참고문헌] - 그림으로 배우는 데이터과학 Data Science - 히사노 료헤이, 키와키 타이치 지음 김성훈 옮김