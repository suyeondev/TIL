# 신경망 기초



## 신경망이란?

신경망은 동물의 신경 시스템을 모방한 학습모델의 총칭이다.

인간을 포함한 동물의 신경 시스템은 시냅스로 불리는 전기적 결합으로 연결된 다수의 신경 세포로 구성된다. 신경 세포는 다른 세포와 달리 세포막 내외의 전위차 변화를 이용하여 서로 정보를 주고받을 수 있다.

시냅스를 매개로 다른 세포로부터 자극을 받아들인 신경세포는 자극이 어느 일정 수위를 넘으면 흥분 상태가 되고, 그 세포가 시냅스 결합을 가진 다른 신경 세포에 자극을 전달한다. 



신경망은 이 신경 세포의 동작을 단순화해서 모방한 뉴런이라는 계산 소자를 다수 결합해서 구성한 학습 모델이다. 뉴런으로서 가장 일반적인 것은 1940년대에 제안된 맥컬록-피츠의 뉴런이다. 이 뉴런은 다수의 입력을 받아서 가중치의 총합이 임계값을 넘엇을 때 흥분 상태를 나타내는 1을 출력하고, 그 이외에는 0을 출력하는 함수이다.

컴퓨터의 연산회로가 다수의 논리 회로를 조합해 다양한 연산을 할 수 있는 것처럼 단순한 계산 소자인 뉴런을 복잡하게 구성함으로서 다양한 계산이 가능해진다.



## 단층 퍼셉트론

복잡한 문제를 해결하는 신경망을 훈련하려면?

1950년대 로젠블라트가 제안한 퍼셉트론 알고리즘이다.

로젠블라트가 이용한 모델은 1980년대에 제안된 다층 퍼셉트론과 비교해서 단층 퍼셉트론이라고 불리기도 한다.



![img](https://t1.daumcdn.net/cfile/tistory/23573D3656C2D8421C)

(이미지 출처 -https://untitledtblog.tistory.com/27)

#### 단층 퍼셉트론 특징

- 계층성
- 입력을 제외하고 2개 층으로 구성된 모델 
- 입력-1층으로의 결합은 랜덤값으로 고정된 것
- 실제 학습 대상이 되는 것은 1층에서 2층으로의 결합 파라미터.
- 2진 분류 문제에 대해 데이터를 하나씩 추출하고, 그 식별 오차를 설명하는 확률적 경사 하강법의 원형이라고 부를 수 있지만, 단층 퍼셉트롭에만 이용할 수 있는 등 일반성이나 범용성이 없다.



퍼셉트론 알고리즘의 중요한 성질?

선형분리가 가능하면 최적해를 발견할 수 있지만, 반대로 선형분리가 불가능한 문제는 어떻게 할 수도 없다는 점.



#### 단층 퍼셉트론의 한계

 

선형분리 불가능한 문제에는 대처할 수 없다. 

![img](http://wiki.hash.kr/images/thumb/6/6c/XOR.png/400px-XOR.png)



단층 퍼셉트론과 같은 간단한 뉴런 연결 방식으로는 이처럼 조금만 어려운 문제도 풀 수 없다는 사실을 1960년대에 들어와서 지적받을면서 1차 신경망 열풍은 사라지게 됐다.





## 다층 퍼셉트론



#### 다층 퍼셉트론이란? 

1980년대에 럼멜하트와 힌튼, 르쿤 등에 의해 생겨난 모델이다. 

입력층, 은닉층, 출력층으로 된 신경망이다.



![img](https://smartstuartkim.files.wordpress.com/2019/02/mlp.png)

(이미지 출처-https://smartstuartkim.wordpress.com/2019/02/02/history-of-neural-networks-2-multilayer-perceptron-mlp/)



다층 퍼셉트론을 이용하면 단층 퍼셉트론으로 불가능했던 문제를 풀 수 있을지도 모른다는 가능성은 1980년대에 2차 신경망 열풍을 일으켰다. 

![img](http://www.cs.ru.nl/~ths/rt2/col/h10/draw-LTUdecis.GIF)

(이미지 출처 - https://coolingoff.tistory.com/7)

이 열풍속에서 특히 중요한 것은 배론 등이 제시한 **"시그모이드 활성화 함수를 가진 다층 퍼셉트론은 충분히 많은 중간층 뉴런을 준비하면 임의의 함수를 임의의 정밀도로 근사할 수 있다."**는 정리이다.



#### 다층퍼셉트론 학습

계단형 활성화 함수 : x가 0일때, 기울기를 계산할 수 없다.

시그모이드 활성화 함수 : 어디에서나 간단히 미분을 실행할 수 있는 함수

![img](https://blog.kakaocdn.net/dn/ebMGVF/btqySjyHtCo/62Xfg5JO7otMYPwRNjrNSk/img.jpg)

(이미지 출처 - https://ppomelo.tistory.com/103)

어느 방향으로 파라미터를 움직이면 목적 함수가 개선되어 학습이 촉진되는지 쉽게 알 수 있기 때문에 시그모이드 활성화 함수를 이용한 신경망은 매우 다루기 쉽다.

![오차역전법_이미지](https://user-images.githubusercontent.com/86271820/156535168-c10452e7-ae71-4a79-9351-3265fd00eb44.PNG)

다층 퍼셉트론에서의 파라미터에 관한 미분을 효율적으로 하는 방법이 오차역전파이다.  파라미터를 각 기어의 회전량이라고 하면, 파라미터에 관한 미분은 각 기어의 출력에 대한 회전비에 해당한다. 회전비는 출력에서 입력을 향해 차례로 각 기어의 회전비를 곱하는 것으로 효율적으로 계산할 수 있다. 입력에서 출력으로 전달되는 회전과는 역방향으로 계산이 진행되므로 역전파라고 불린다.



#### 다층 퍼셉트론의 한계



1980년대의 2차 신경망 열풍도 오래가진 못했다. 그 이유는 신경망이 갖는 몇가지 근본적인 문제와 당시 기술적인 한계에서 기인한다.



근본적인 문제 : **다층 퍼셉트론의 목적함수의 비볼록성**이 있다.

1. 비볼록성은 **경사법으로 최적해를 구하기가 사실상 불가능**하고, 좋은 국부 최적해를 얻기 위해서는 원리상 많은 시행이 필요하다는 것을 의미한다. 
2. **기울기 소실, 폭발 문제** 등 학습 자체가 어렵다.
3. **과적합**하기가 쉽다.
   - 일반적으로 머신러닝의 복잡성을 늘리면, 데이터에 포함된 본질적이지 않은 노이즈에 끌려다니기 쉬운 경향이 있다.
   - 은닉층 뉴런을 충분히 준비하면 어떤 함수라도 표현할 수 있다고 설명했지만, 이는 적절히 정규화하지 않을 경우 간단히 과적합해 버린다는 것 의미한다.



1990년대 이후 당시 높은 정밀도를 내던 서포트 벡터 머신(SVM) 등으로 대체된다.



[참고문헌] - 그림으로 배우는 데이터과학 Data Science - 히사노 료헤이, 키와키 타이치 지음 김성훈 옮김
